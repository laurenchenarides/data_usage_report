[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "How to Cite:\n\n\n\nChenarides, L., Bryan, C., & Ladislau, R. (2025). Methodology for comparing citation database coverage of dataset usage. Available at: https://laurenchenarides.github.io/data_usage_report/report.html\nDownload PDF Version",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#what-is-the-issue",
    "href": "report.html#what-is-the-issue",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "What Is the Issue?",
    "text": "What Is the Issue?\nFederal datasets play an important role in supporting research across a range of disciplines. Measuring how these datasets are used can help evaluate their impact and inform future data investments. Agencies like the US Department of Agriculture (USDA) track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like Democratizing Data’s Food and Agricultural Research Data Usage Dashboard and NASS’s 5 W’s Data Usage Dashboard. These tools rely on identifying dataset mentions1 in published research to develop usage statistics. Beyond reporting usage statistics, this type of analysis can also provide information about the research topics where federal datasets are applied. Understanding how federal datasets are applied helps characterize their disciplinary reach, including use in areas such as food security, nutrition, and climate, which are inherently multidisciplinary. This informs future work on identifying alternative datasets that researchers use to study similar questions across fields.\nThe process of identifying dataset mentions in academic research output has two requirements. First, citation databases provide structured access to large volumes of publication metadata, including titles, abstracts, authors, affiliations, and sometimes full-text content. Second, tracking dataset usage requires developing methods that scan publication text for dataset mentions. It is feasible to systematically identify where specific datasets are referenced across a broad set of research outputs by applying machine-learning algorithms to publication corpora collected from citation databases, allowing for scalable search and retrieval of relevant publications where datasets are mentioned. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. However, different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports - and dataset tracking requires reliable citation data from citation databases.\nThis report presents a systematic review of identifying dataset mentions in research publications across various citation databases. In doing so, we compare publication, journal, and topic coverage across Scopus, OpenAlex, and Dimensions as primary sources. The purpose is to establish a consistent set of statistics for comparing results and evaluating differences in dataset tracking across citation databases. This allows for insights into how publication scope and indexing strategies influence dataset usage statistics.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#how-was-the-study-conducted",
    "href": "report.html#how-was-the-study-conducted",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "How Was the Study Conducted?",
    "text": "How Was the Study Conducted?\nThree citation databases are compared: Elsevier’s Scopus, OurResearch’s OpenAlex, and Digital Science’s Dimensions.ai.\n\nScopus charges for access to its citation database. It indexes peer-reviewed, including journal articles, conference papers, and books, and provides metadata on authorship, institutional affiliation, funding sources, and citations. For this study, Scopus was used to identify dataset mentions through a two-step process: first, Elsevier executed queries against the full-text ScienceDirect corpus and reference lists within Scopus; second, publications likely to mention USDA datasets were filtered based on keyword matching and machine learning models.\nOpenAlex, an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports. In this study, we used two approaches to identify dataset mentions in OpenAlex: a full-text search, which scans publication metadata fields such as titles and abstracts for references to USDA datasets,2 and a seed corpus search, which starts with a targeted set of publications based on journal, author, and topic criteria, then downloads the full text of each paper to identify mentions of USDA datasets.3\nDimensions, developed by Digital Science, is a citation database that combines free and subscription-based access. It indexes a range of research outputs, including journal articles, books, clinical trials, patents, datasets, and policy documents. Dimensions also links publications to grant and funding information. For this study, publications in Dimensions that reference USDA datasets were identified by constructing structured queries in Dimensions’ Domain Specific Language (DSL) that combined dataset aliases with institutional affiliation terms. These were executed via the dimcli API to return English-language articles from 2017–2023 with at least one U.S.-affiliated author. To maintain consistency with the criteria applied to Scopus and OpenAlex, the study focuses only on publications classified as journal articles.\n\nTo compare how these databases track dataset usage, we focus on six USDA datasets commonly used in agricultural, economic, and food policy research:\n\nAgricultural Resource Management Survey (ARMS)\nCensus of Agriculture (Ag Census)\nRural-Urban Continuum Code (RUCC)\nFood Access Research Atlas (FARA)\nFood Acquisition and Purchase Survey (FoodAPS)\nHousehold Food Security Survey Module (HHFSS)\n\nThese datasets were selected for their policy relevance, known usage frequency, and disciplinary breadth. We developed seed corpora for each dataset to identify relevant publications, then used those corpora to evaluate database coverage, topical scope, and metadata consistency.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#what-did-the-study-find",
    "href": "report.html#what-did-the-study-find",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "What Did the Study Find?",
    "text": "What Did the Study Find?\nTracking dataset mentions varies significantly depending on which citation database is used. This analysis compares Scopus, OpenAlex, and Dimensions to determine how each citation database captures research mentioning key USDA datasets.\nKey Findings Across Sources:\n\nPublications:\n\nOverlap across databases is limited. For most datasets, fewer than 10% of DOIs appear in all three sources. Scopus often identifies the largest share of indexed DOIs, especially for public health–related datasets. OpenAlex captures a broader set of publication types, including preprints and working papers. Dimensions often sits in the middle but includes the highest number of matched DOIs for some datasets.\n\nJournals:\n\nScopus emphasizes disciplinary journals, particularly in health, economics, and social science. OpenAlex includes a mix of traditional and nontraditional outlets, including open-access platforms. Dimensions covers many of the same journals as Scopus but with a stronger presence of applied policy and public health titles.\n\nTopics:\n\nWhile the same datasets appear across all three sources, the topical classifications differ.\n\nARMS is associated with farm management, production economics, and sustainability.\nCensus of Agriculture connects to agricultural structure, environmental policy, and rural development.\nFood Access Research Atlas highlights food security, neighborhood-level inequality, and planning.\nFoodAPS centers on household behavior, SNAP, and diet cost.\nHFSSM is tied to poverty, food insecurity, and health disparities.\nRUCC connects to rural healthcare, regional planning, and demographic trends.\n\nEach source applies a different classification system, which affects how these themes are surfaced and grouped.\n\nAuthors:\n\nScopus and Dimensions tend to recover more academic authors in applied economics, public health, and nutrition. OpenAlex often identifies a wider array of author types. Across sources, many of the most active authors are affiliated with USDA Economic Research Service, major land-grant universities, and schools of public health.\n\nInstitutions:\n\nInstitutional representation varies, with Scopus and Dimensions surfacing more authors from top-tier research universities and federal agencies. OpenAlex includes more community-based organizations and international institutions not always indexed in Scopus.\nEvaluating Corpus Coverage Across Sources\nAmong the three sources examined, Dimensions offered the most consistently structured metadata linking datasets to publications. Its combination of broad journal coverage, funder metadata, and curated topic tags allowed for easier identification of research that referenced USDA datasets, particularly in applied and policy-relevant contexts.\nAlthough Scopus recovered the largest number of publications for certain datasets and fields, and OpenAlex captured a wider range of publication types (including international and open source journals), Dimensions provided the most streamlined path to assembling a usable corpus with fewer manual adjustments. This made it especially useful for mapping the reach of a dataset across disciplines and institutions.\nUltimately, each source contributed unique value to the analysis, and comparing across systems helped surface important differences in coverage and classification.\nTakeaway:\nNo single citation database captures the full scope of research publications referencing USDA datasets. Differences in indexing practices, topic labeling, and metadata structure shape what research is discoverable and how it is interpreted. Among the sources evaluated, Dimensions provided the most consistent, policy-relevant, and accessible view of dataset usage making it a strong candidate for future efforts to track the reach and impact of publicly funded data.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#how-to-use-this-report",
    "href": "report.html#how-to-use-this-report",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "How to Use This Report",
    "text": "How to Use This Report\nThis report outlines an initial approach for characterizing how USDA-related food and agriculture datasets are referenced in research publications indexed by Scopus, OpenAlex, and Dimensions. The work is not peer-reviewed but is fully transparent and reproducible, with all underlying code and procedures available for verification and reuse.\nThe report includes methods for:\n\nIdentifying publication coverage across citation databases\nCross-referencing dataset mentions across sources\nAnalyzing research topics, institutional affiliations, and author networks\n\nReusable components produced as part of this effort include:\n\nA code repository for data cleaning and standardization\nA crosswalk of data schemas by citation database\n\nThe general framework developed here can be extended to other citation systems, including Web of Science, Crossref, and Microsoft Academic, for similar evaluations of dataset coverage and usage.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#project-background",
    "href": "report.html#project-background",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.1 Project Background",
    "text": "2.1 Project Background\nTracking how federal datasets are used in academic research has been a priority for agencies such as the U.S. Department of Agriculture (USDA). Democratizing Data’s Food and Agricultural Research (FAR) Data Usage Dashboard was developed to support this effort by identifying and counting publications referencing USDA datasets. Initially built on Scopus, a proprietary citation database with structured indexing and reliable metadata, the dashboard faced limitations due to access costs and restricted journal availability.\nAs interest in open-access infrastructure has grown, OpenAlex, a free and open-source citation database developed by OurResearch, has emerged as a potential alternative. OpenAlex offers broad coverage of research outputs, including preprints and conference proceedings, and has attracted attention as a scalable replacement for proprietary systems. However, switching platforms raises questions about coverage completeness, data reliability, and how well each database supports transparent monitoring of dataset use.\nIn parallel with this evaluation, a new partnership was formed with Digital Science, the developers of Dimensions. Dimensions offers a hybrid model of free and subscription-based services and provides API access that facilitates structured identification of dataset mentions. Compared to other platforms, Dimensions includes grant metadata, standardized topic taxonomies, and curated dataset linkages, helping overcome several limitations identified in Scopus and OpenAlex.\nAlthough USDA discontinued its direct support for the dashboard, this work was taken up by the National Data Platform as part of a broader effort to build trusted infrastructure for data-driven research. To inform this transition, a systematic comparison was conducted across Scopus, OpenAlex, and Dimensions to assess their relative strengths for tracking dataset usage in food and agricultural research. The goal was not to endorse a single platform, but to provide a transparent and replicable framework for evaluating citation data quality, coverage, and relevance for public data monitoring.\n\n2.1.1 Project Objective\nThis report presents a method for tracking how six key USDA datasets (Table 1) are mentioned in research using Scopus, OpenAlex, and Dimensions. It identifies where each dataset appears, which topics they are used in, which authors and institutions are most active, and how these patterns vary depending on the citation database. The findings reveal how differences in database coverage and classification can affect assessments of dataset use.\n\n\n2.1.2 Specific Aims\nThis section outlines the core objectives guiding the database comparison and the steps used to determine how well each citation platform captures publications that mention key USDA datasets.\n\nEvaluate differences in publication coverage across citation databases. Measure the extent to which Scopus, OpenAlex, and Dimensions capture research publications that reference USDA datasets. Identify how publication inclusion varies across platforms.\nCompare journal indexing and scope. Compare the journals indexed by each database and examine how differences in journal coverage influence visibility of dataset-linked research.\nAnalyze topic coverage. Examine the research areas where USDA datasets are mentioned. Identify patterns in topic classification and assess how different citation databases support subject-level tracking of dataset usage.\nEvaluate author representation. Compare how author names are recorded across platforms, including the completeness of author metadata and potential implications for attribution and visibility.\nExamine institutional representation. Evaluate how each platform captures and standardizes institutional affiliations. Pay particular attention to differences in coverage for Minority-Serving Institutions (MSIs), land-grant universities, and other public or underrepresented institutions.\nDevelop a reproducible methodology for cross-platform comparison. Create a generalizable workflow for comparing citation databases, including steps for record linkage, deduplication, author and institution standardization, and identification of dataset mentions.\n\nThe methodology described in this report provides a systematic approach for comparing publication coverage where federal datasets are mentioned across citation databases. The scope of work includes comparing publication coverage across Scopus, OpenAlex, and Dimensions. For more information on the metadata available from each citation database, refer to this Appendix. These methods can be applied to other citation databases as alternatives to current data sources.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#data-collection",
    "href": "report.html#data-collection",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.2 Data Collection",
    "text": "2.2 Data Collection\nA core objective of this study is to evaluate publication coverage across citation databases, focusing on how well Scopus, OpenAlex, and Dimensions index research relevant to food and agricultural research. A targeted strategy was used to identify publications referencing USDA datasets, aligning with federal agency efforts to monitor and report on dataset usage. This approach enables a consistent entry point for comparison across platforms while also providing insight into the topics where federal datasets are applied and the use of complementary or alternative data sources.\nTo support this analysis, a structured inventory of USDA data assets was developed, drawing from records produced by the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). From this broader inventory, six datasets were selected for detailed comparison based on known usage, policy relevance, and disciplinary breadth: the Census of Agriculture, Agricultural Resource Management Survey (ARMS), Food Acquisition and Purchase Survey (FoodAPS), Food Access Research Atlas (FARA), Rural-Urban Continuum Code (RUCC), and the Household Food Security Survey Module (HFSSM). The set of data assets, their producing agencies, and descriptions are presented in Table 1.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\n\n\n\n\nResearchers reference datasets in inconsistent ways—using acronyms, abbreviations, alternate spellings, or related URLs. To capture these variations, we created a structured list of dataset–alias pairs, called dyads. This Appendix provides the full list of dyads used to search for mentions of each USDA dataset across Scopus, OpenAlex, and Dimensions. This list ensures consistent and comprehensive identification of dataset mentions in research publications.\nUsing these dyads, we applied tailored search strategies across each citation database to identify relevant publications for all six datasets. These included a seed search in Scopus, a full-text metadata search in OpenAlex, a seed corpus approach in OpenAlex based on targeted filtering of journals, authors, and topics followed by full-text analysis, and a full-text search in Dimensions. Each search strategy is described in detail in the following sections.\n\n2.2.1 Scopus Approach\nThe first citation database used is Scopus, a publication catalog managed by Elsevier. Ideally, direct Scopus API access would have been used to query full publication text for mentions of USDA datasets. However, the project did not have access to the Scopus API. Only Elsevier, serving as a project partner, was able to execute queries within the Scopus environment. Consequently, the dataset mention search relied on outputs provided by Elsevier rather than independent querying.\nBecause of these constraints, a seed corpus approach was applied. First, Elsevier matched the names and aliases of all USDA datasets against full-text records available through ScienceDirect and reference sections of Scopus publications published between 2017 and 2023. This initial step identified journals, authors, and topics most likely to mention USDA datasets. A targeted search corpus was then constructed, narrowing the scope to approximately 1.45 million publications. These included various document types—articles, reviews, short surveys, notes, conference papers, chapters, books, editorials, letters, data papers, errata, and tombstones. For the purposes of this comparative report, only articles are considered.\nSeveral methods were used to identify mentions of USDA datasets in Scopus publications. First, a reference search was conducted, using exact-text matching across publication reference lists to capture formal citations of datasets. Second, full-text searches were performed using machine learning models applied to publication bodies, identifying less formal mentions of datasets. Third, machine learning routines developed through the 2021 Kaggle competition were applied to the full-text corpus to improve detection of dataset mentions, including instances where references were indirect or less structured. Details about the three machine learning models used are available here.\nBecause direct access to full publication text was not available, Elsevier shared only the extracted snippets and limited metadata. Manual validation, aided by the use of keyword flags (e.g., “USDA,” “NASS”), confirmed whether identified mentions accurately referred to the targeted datasets. To manage validation costs, only publications with at least one U.S.-based author were reviewed.\nFull documentation of the Scopus search routine, including query construction and extraction procedures, is available in this appendix..\n\n\n2.2.2 OpenAlex Approach\nThe second citation database used is OpenAlex, an open catalog of scholarly publications that provides public acess to metadata and, when available, full-text content for open-access publications via its API. Unlike Scopus, which provides controls access to licensed content, OpenAlex indexes only open-access publications or those for which open metadata has been made available by publishers.\nTwo methods were used to identify USDA dataset mentions in OpenAlex: a full-text search and a seed corpus approach. Both methods focused on peer-reviewed journal articles published between 2017 and 2023 and restricted the dataset to final published versions, excluding preprints and earlier drafts to avoid duplication across versions.\n\n2.2.2.1 Method 1: Full-Text Search\nThis method relied on querying OpenAlex’s full-text search index using combinations of dataset aliases (e.g., alternate names, acronyms) and institutional flag terms (e.g., “USDA,” “NASS”). The combination of dataset alias and flag terms ensured that retrieved publications made an explicit connection to the correct data source. A “true” dataset mention was recorded only when at least one alias and one flag term appeared in the same publication, increasing the precision of captured dataset mentions.4\nQueries were implemented using the pyalex Python package5, which manages API requests and enforces OpenAlex’s usage rate limits. The search used the search and filter endpoints, targeting English-language, open-access articles or reviews published after 2017. Results were returned in JSON format based on the OpenAlex Work object schema, including fields for publication metadata, authorship, journal, concepts, citations, and open access status. Each record included metadata fields such as:\n\ndisplay_name (publication title)\nauthorships (authors and affiliations)\nhost_venue.display_name (journal)\ndoi (digital object identifier)\nconcepts (topics)\ncited_by_count (citation counts)\ntype (publication type, e.g., “article”)\npublication_year (year article was publish)\nlanguage (language, English only)\nis_oa (open access)\n\nFull documentation of the OpenAlex search routine, including query construction and extraction procedures, is available in this appendix..\n\n2.2.2.1.1 Limitations of Full-Text Search Method\nAlthough the OpenAlex API provides access to full-text search, limitations in content ingestion affect result completeness. OpenAlex receives publication text through two primary ingestion methods: PDF extraction and n-grams delivery.\nIn the PDF ingestion method, OpenAlex extracts text directly from the article PDF. However, the references section is not included in the searchable text. References are processed separately to create citation pointers between scholarly works, meaning that mentions of datasets appearing only in bibliographies are not discoverable through full-text search.\nIn the n-grams ingestion method, OpenAlex does not receive the full article text. Instead, it receives a set of extracted word sequences (n-grams) from the publisher or author. These n-grams represent fragments of text—typically short sequences of one, two, or three words—which are not guaranteed to preserve full continuous phrases. As a result, complete dataset names may be broken apart or omitted, reducing the likelihood that search queries match the intended aliases.\nThese ingestion and indexing limitations affect the completeness of results when relying solely on OpenAlex full-text search. Mentions of USDA datasets that appear either exclusively in references or are fragmented within n-grams may be missed. To address these limitations, an alternative search method was developed based on constructing a filtered seed corpus of publications for local full-text analysis.\n\n\n\n2.2.2.2 Method 2: Seed Corpus\nTo overcome the limitations of the full-text metadata search, a seed corpus approach was developed. This method created a filtered subset of publications for local full-text analysis, targeting likely mentions of USDA datasets.\nSelection criteria for the seed corpus included:\n\nEnglish-language publications\nWorks published between 2017-2023\nPublication Type = articles\nOpen-access publications only\n\nTo focus the sample, we used results from the initial OpenAlex full-text search to identify the top 25 journals, authors, and topics most frequently associated with USDA dataset mentions. For each entity, we computed a Full-Text Search Count, which is the number of publications where USDA datasets were explicitly mentioned in the full text. This metric reflects how often each topic, journal, or author has appeared in USDA dataset–relevant research.\nWe then filtered the broader OpenAlex catalog to include all publications—regardless of whether they mentioned a dataset—linked to these top-ranked entities. This allowed us to build a more focused but expansive corpus for local text search. By narrowing to 25 entities per category, we prioritized relevance while managing scale. This process generated a structured set of JSON files containing publication metadata and links. The Python script used to flatten and process these files is provided in this appendix.\nExample: Census of Agriculture\nTo illustrate this process, consider the tables created for the Census of Agriculture dataset—Table 4 (top 25 topics), Table 5 (top 25 journals), and Table 6 (top 25 U.S.-affiliated authors). Each table contains two columns:\n\nFull-Text Search Count: Number of publications from the OpenAlex full-text search that mention the dataset and are linked to the given topic, journal, or author\nTotal Count: Total number of publications in OpenAlex associated with that topic, journal, or author, regardless of dataset mention\n\nThe Full-Text Search Count helps us identify which entities are most directly associated with USDA dataset use. For instance, if a topic like “Impact of Food Insecurity on Health Outcomes” has 78 dataset-related publications. This count reflects how often USDA datasets were mentioned within the full text of publications associated with a particular entity. Meanwhile, the OpenAlex Total Count shows the broader publication volume for that topic—in this case, over 78,000—providing context on how prominent the topic is within the full OpenAlex database. In this sense, the Full-Text Search Count serves as a rough proxy for market penetration, or how frequently a dataset appears within a given research area relative to the total volume of publications.\nThe Full-Text Search Count reflects how often USDA datasets are explicitly mentioned within a specific research area, while the Total Count represents the overall volume of publications linked to that topic, journal, or author. The large gap between these counts was a key reason for developing the seed corpus approach: even within high-relevance entities, many publications may reference datasets in ways not captured by OpenAlex’s full-text search.\nBy downloading and analyzing the full texts of all publications linked to the entities in the second column, we applied our own string-matching logic to detect mentions that OpenAlex’s indexing may have missed, particularly in reference sections or when dataset names were fragmented. This allowed us to validate and extend OpenAlex search results using a consistent and transparent local method.\nThis approach has several implications. It increases the relevance of the corpus by focusing on publications where USDA datasets are actively cited, rather than broadly associated with a topic. It also reduces processing demands by avoiding the need to download all potentially relevant PDFs. However, by prioritizing high-visibility entities from the initial search, the method may introduce selection bias and miss less frequently cited but still relevant work. The trade-off reflects a practical balance between analytical depth and operational feasibility.\nFor the Census of Agriculture, the resulting seed corpus included approximately 1.77 million unique publications. About 35% of full texts were successfully downloaded, yielding an estimated 625,000 documents for local analysis. Full-text searches on this subset improved detection of dataset mentions beyond what OpenAlex’s native indexing allowed.\nDespite the benefits, limitations remain. Full-text availability was constrained by broken or inaccessible links, and processing the corpus was computationally intensive. Future work may require distributed processing or more refined filters to improve efficiency.\nThe table below summarizes primary differences between the Full-Text Search and Seed Corpus methods. The Full-Text Search provides broader initial coverage, but it is limited by indexing constraints and lack of reference section access. The Seed Corpus narrows the search space but allows for deeper, locally controlled analysis of full-text content, including citations.\n\n\n\nTable 2: Key Differences Between OpenAlex Full-Text Search and Seed Corpus\n\n\n\n\n\n\n\n\n\n\nFeature / Criterion\nFull-Text Search\nSeed Corpus\n\n\n\n\nSearchable Sample \nOpenAlex API where has_fulltext = true\nCurated list based on known users/sources\n\n\nSource of text\nArticle body or word/phrase snippets where fulltext_origin = n-grams\nAny part of publication conditional on available PDF download\n\n\nReference sections indexed?\nNo\nYes. Will include publications that reference datasets in citations.\n\n\nFull text required? (has_fulltext)\nYes\nNot required\n\n\nOpen access required? (is_oa)\nNo\nYes. Method requires downloading the full PDF version of the article.\n\n\nSelection criteria\nNone imposed a priori\nJournal/topic/author targeting\n\n\nResulting sample\nBroad, but with limitations\nNarrower, given the target search criteria\n\n\n\n\n\n\n\n\n\n2.2.3 Dimensions\nTo identify publications mentioning USDA datasets, we used the Dimensions.ai API, following the same general methodology applied in Scopus and OpenAlex. We reused the same dataset aliases, institutional flag terms, and overall search criteria to ensure consistency across sources. The search covered scholarly publications from 2017 to 2023 and was restricted to works authored by at least one researcher affiliated with a U.S.-based institution.\nDimensions queries are written using a structured Domain Specific Language (DSL). We constructed boolean queries that combined multiple dataset aliases (e.g., “NASS Census of Agriculture”, “USDA Census”, “Agricultural Census”) with institutional identifiers (e.g., “USDA”, “NASS”, “U.S. Department of Agriculture”). As with Scopus and OpenAlex, both a dataset alias and an institutional flag term were required to appear in each result. These terms were grouped using OR within each category and then combined with an AND across categories. For example:\n\n(“NASS Census of Agriculture” OR “Census of Agriculture” OR “USDA Census of Agriculture” OR “Agricultural Census” OR “USDA Census” OR “AG Census”) AND (USDA OR “US Department of Agriculture” OR “United States Department of Agriculture” OR NASS OR “National Agricultural Statistics Service”)\n\nWe implemented this process using the dimcli Python library, which provides a streamlined interface to the Dimensions.ai API and automates result pagination. A significant advantage of this approach is the capability of the Dimensions.ai platform to manage complex searches directly, resulting in precise results and reduced computational overhead. By executing these queries directly through the API, we avoided the technical complexity associated with downloading and locally processing large amounts of textual content. Moreover, the Dimensions.ai API results can be automatically structured into an analysis-ready DataFrame format. This simplified data structure greatly facilitated our subsequent validation, data integration, and analytical workflows.\nTo maintain methodological consistency with Scopus and OpenAlex, the following filters were applied to the search:\n\nEnglish-language publications\nWorks published between 2017-2023\nDocument types: articles, chapters, proceedings, monographs, and preprints\nAuthor affiliations: Publications were filtered to include only those authored by researchers affiliated with at least one U.S.-based institution.\n\nFor comparability with the Scopus and OpenAlex samples, only publications classified as “articles” were retained for final analysis. This restriction reduces duplication across versions (e.g., preprints, proceedings) and reflects our focus on peer-reviewed scholarly output.\nFor each article, we retrieved metadata including title, authors, DOI, journal, abstract, publication date, citation counts, subject classifications, and links. These fields supported topic-level analysis, author and institution mapping, and validation of dataset mentions.\nUsing Dimensions.ai provided two main technical advantages. First, because the platform supports full-text query execution natively, we avoided the need to download or parse external files. Second, the API responses were easily converted into analysis-ready DataFrames, which simplified downstream validation and integration with other sources.\nOverall, the Dimensions.ai approach aligned with our methods for Scopus and OpenAlex, enabling consistent identification of USDA dataset mentions across all three platforms.\n\n\n2.2.4 Data Processing\nTo produce a consistent count of unique publications referencing each USDA dataset, records from three sources-Scopus, OpenAlex, and Dimensions-were consolidated, each of which identified publications through a different mechanism, described above.\nFor each source, publication-level metadata, including DOIs, journal titles, ISSNs (when available), and source-specific topic classifications was extracted. DOIs were standardized (e.g., removing URL prefixes, https://doi.org/) for consistent matching across sources. Duplicate DOIs within each source were removed. All DOIs compared in this report are associated with publications classified as document type = article and were published between 2017 and 2023.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#results",
    "href": "report.html#results",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.3 Results",
    "text": "2.3 Results\nThe aims described in Section 2.1.2 guide the development of a methodology for comparing citation databases, focusing on four areas:\n\nPublication tracking: Comparing how each platform captures publications within indexed journals\nJournal coverage: Determining which journals each platform indexes\nTopic scope: Evaluating the research areas of publications that cite USDA datasets\nAuthor and institutional affiliation: Determining how each platform records institutional information\n\nProcessed publication metadata was then merged across sources using the cleaned DOI-ISSN pairs as the common identifier. Each publication was tagged with binary indicators showing whether it appeared in Scopus, OpenAlex Full Text, OpenAlex Seed, or some combination thereof. When metadata overlapped (such as journal titles or publication years), Scopus information was prioritized, when available, given its relatively higher metadata quality, followed by OpenAlex Full Text, OpenAlex Seed, and then Dimensions.6\nThis process ensured that each publication was counted once, even if it appeared in multiple sources. The final dataset includes a deduplicated set of DOIs, along with harmonized metadata and source indicators. The number of unique publications referencing each dataset is shown in Table 3.\n\n\n\nTable 3: Unique Publications with Metadata across Sources\n\n\n\n\n\nDataset Name\nNumber of Unique Publications\n\n\n\n\nARMS\n1,581\n\n\nCensus of Agriculture\n5,835\n\n\nFood Access Research Atlas\n590\n\n\nFood Acquisition and Purchase Survey\n808\n\n\nHousehold Food Security Survey Module\n1,408\n\n\nRural-Urban Continuum Code\n2,215\n\n\n\n\n\n\nAll code used to clean, deduplicate, and merge records is provided in the GitHub repository.\n\n2.3.1 Publication Coverage\nAn objective of this report is to understand differences in publication coverage across Scopus, OpenAlex, and Dimensions. Specifically, this section asks: (1) how many and which publications referencing USDA datasets appear in each citation database, and (2) how many and which journals publishing these articles overlap between the two sources.\nIn addition, the analysis evaluates whether the different search strategies used in OpenAlex—the full-text metadata search versus the seed-corpus approach—yield substantially different sets of results.\nFor each of the six USDA datasets (Table 1) featured in this study, a treemap visualization summarizes publication coverage across the three citation databases. Each treemap groups publications into mutually exclusive categories based on their presence in one or more of the sources. The size of each box is proportional to the number of distinct DOIs in that group, providing a visual summary of relative coverage. For example, a large “Scopus only” segment indicates a high number of publications indexed exclusively in Scopus, while overlapping segments (e.g., “Scopus ∩ Dimensions”) reflect shared coverage between platforms.\n\nAgricultural Resource Management Survey (ARMS)\nOpenAlex dominates coverage for ARMS-related publications, capturing nearly 76% of all distinct DOIs exclusively. In contrast, Scopus and Dimensions contribute relatively little: just 8.9% and 5.6% of DOIs appear exclusively in those sources, respectively. Overlaps are modest, with 2.5% of DOIs shared by OpenAlex and Dimensions, and only 1.9% captured by all three. This suggests OpenAlex’s broader indexing of ARMS publications relative to the other databases.\n\n  \n\n\n\nThe Census of Agriculture\nScopus provides the broadest exclusive coverage for the Census of Agriculture, accounting for 41.3% of DOIs. Dimensions follows at 18.8%, while OpenAlex accounts for just 9.2% exclusively. The largest overlap is between Scopus and Dimensions (22.8%), with limited three-way overlap (3.6%). These results indicate that Scopus and Dimensions are the primary sources capturing publications referencing this dataset.\n\n  \n\n\n\nFood Access Research Atlas\nCoverage for this dataset is more evenly distributed. Scopus and Scopus ∩ Dimensions each account for about 24%, while OpenAlex-only coverage is 14.6%, and Dimensions-only is 11.9%. Notably, 11% of DOIs appear in all three sources. This more balanced distribution suggests broader and more consistent indexing across platforms, without a single source dominating.\n\n  \n\n\n\nThe Food Acquisition and Purchase Survey (FoodAPS)\nOpenAlex again provides the widest exclusive coverage (46.7%), while Scopus and Scopus ∩ Dimensions each contribute 17.8%. Dimensions-only coverage is modest (7.7%), and 4.7% of DOIs are shared across all three. This indicates that OpenAlex is especially important for capturing FoodAPS-related work, but combined use of all three sources increases overall visibility.\n\n  \n\n\n\nThe Household Food Security Survey Module\nScopus has the highest exclusive coverage (34.7%), followed by Scopus ∩ Dimensions (22.3%) and Dimensions-only (17.5%). OpenAlex-only coverage is lower at 12.8%, and just 5.8% of DOIs are indexed by all three. This indicates stronger coverage for HFSSM-related publications in Scopus and Dimensions compared to OpenAlex.\n\n  \n\n\n\nRural-Urban Continuum Code\nCoverage is again led by Scopus (30.8%) and Dimensions (25%), with Scopus ∩ Dimensions contributing another 24.2%. OpenAlex-only coverage is relatively low at 8.1%, and only 5.9% of DOIs are shared across all three. This pattern is consistent with datasets where OpenAlex’s coverage is more limited.\n\n  \n\n\n\n\n\n\n\nSynthesis of DOI Coverage by Source (Percent of Total DOIs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nTotal DOIs\nScopus only (%)\nOpenAlex only (%)\nDimensions only (%)\nScopus ∩ OpenAlex (%)\nScopus ∩ Dimensions (%)\nOpenAlex ∩ Dimensions (%)\nAll three (%)\n\n\n\n\nARMS\n1581\n8.9\n75.8\n5.6\n0.7\n4.6\n2.5\n1.9\n\n\nCensus of Agriculture\n5835\n41.3\n9.2\n18.8\n1.6\n22.8\n2.8\n3.6\n\n\nFood Access Research Atlas\n590\n24.4\n14.6\n11.9\n5.1\n24.7\n8.3\n11.0\n\n\nFoodAPS\n808\n17.8\n46.7\n7.7\n1.7\n17.8\n3.6\n4.7\n\n\nHFSSM\n1408\n34.7\n12.8\n17.5\n2.2\n22.3\n4.6\n5.8\n\n\nRUCC\n2215\n30.8\n8.1\n25.0\n2.3\n24.2\n3.7\n5.9\n\n\n\n\n\n\n\n\n2.3.2 Journal Coverage\nThe previous section documented substantial variation in publication coverage across Scopus, OpenAlex, and Dimensions. One potential explanation for these differences is variation in journal indexing across sources. This section examines that possibility by looking at journal coverage, specifically, whether each citation database indexes the journals where USDA dataset-related publications appear.\nFor each dataset, the analysis identifies the top 40 journals (by DOI count) and determines which citation databases index them. Sankey diagrams illustrate the relationship between citation databases (left) and journals (right). Flows indicate coverage, with journals indexed in multiple sources connected to each. While only the top 40 journals are visualized, a complete list is available in the GitHub repository.\n\nAgricultural Resource Management Survey (ARMS)\nMost top journals referencing ARMS are indexed by OpenAlex, including several high-DOI outlets such as Applied Economic Perspectives and Policy and the American Journal of Agricultural Economics. Fewer journals are exclusive to Scopus or Dimensions. This pattern aligns with OpenAlex’s dominant coverage of ARMS publications in the previous section.\n\n  \n\nClick to enlarge\n\n\n\n\nThe Census of Agriculture\nJournal coverage for Census-related publications is distributed more evenly across the three sources. Several journals—particularly in environmental and remote sensing fields—are indexed only in Scopus or Dimensions. Shared indexing is common for journals like Food Policy and Agricultural Systems, helping to explain the high level of overlap between Scopus and Dimensions.\n\n  \n\nClick to enlarge\n\n\n\n\nFood Access Research Atlas\nThis dataset is associated with journals that are broadly indexed across sources. Titles such as the Journal of Agricultural and Applied Economics and Ecological Economics are covered in all three databases. The strong overlap in journal indexing corresponds with the relatively balanced publication coverage observed in the prior section.\n\n  \n\nClick to enlarge\n\n\n\n\nThe Food Acquisition and Purchase Survey (FoodAPS)\nMany FoodAPS-related journals fall within the nutrition and behavioral sciences domains, and several of these—such as Appetite and Frontiers in Nutrition—are indexed in OpenAlex. While a subset of journals is also covered by Scopus and Dimensions, OpenAlex appears to index more of the high-volume titles, consistent with its higher share of FoodAPS-related DOIs.\n\n  \n\nClick to enlarge\n\n\n\n\nThe Household Food Security Survey Module\nThis dataset draws from a wide range of journals in public health, food policy, and applied economics. Journals such as Food Security and Journal of Nutrition Education and Behavior appear in all three sources, but some health-focused titles are only indexed in Scopus or Dimensions. These differences likely contribute to the stronger coverage seen in Scopus and Dimensions.\n\n  \n\nClick to enlarge\n\n\n\n\nRural-Urban Continuum Code\nJournals citing RUCC span health, epidemiology, and rural development. Many are indexed in Scopus and Dimensions, including Environmental Research, BMC Public Health, and Drug and Alcohol Dependence. OpenAlex has more limited coverage of these titles, consistent with its lower representation of RUCC-related DOIs.\n\n  \n\nClick to enlarge\n\n\n\n\n\n\n\n\nSummary of Journal Coverage by Dataset\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nDominant Source\nNotable Journals Indexed in All Sources\nNotable Journals Missing from Some Sources\n\n\n\n\nARMS\nOpenAlex\nAJAE, AEPP, Agribusiness\nFew missing; OpenAlex covers most top journals\n\n\nCensus of Agriculture\nScopus / Dimensions\nFood Policy, Agricultural Systems\nEnvironmental/remote sensing journals missing in OpenAlex\n\n\nFood Access Research Atlas\nShared\nJAAEA, Ecological Economics\nBroad overlap; minimal gaps\n\n\nFoodAPS\nOpenAlex\nFood Security, Frontiers in Nutrition\nSome nutrition journals missing in Scopus/Dimensions\n\n\nHFSSM\nScopus / Dimensions\nJNED, Food Security\nSome public health journals missing in OpenAlex\n\n\nRUCC\nScopus / Dimensions\nEnvironmental Research, Food Policy\nSeveral epidemiology/health titles missing in OpenAlex\n\n\n\n\n\n\n\n\n2.3.3 Publication Topics\nIn addition to differences in coverage and journal indexing, citation databases vary in how they classify research content. Each system applies a distinct taxonomy—often algorithmically generated—to assign topics to publications. These systems function like thematic filters, shaping how research is organized, discovered, and interpreted.\nTo understand how topic classification differs across sources, this section compares the most frequent topics assigned to the same set of publications by Scopus, OpenAlex, and Dimensions.\nWhy Focus on Overlapping Publications?\nTo ensure comparability, the analysis is restricted to DOIs that appear in all three databases. This approach isolates differences in classification by holding the underlying publication set constant. Any observed variation reflects how each database labels and groups the same publications.\nWord Cloud Construction\nFor each dataset, the word clouds are based on frequency tables constructed from topic metadata assigned by each source. Specifically:\n\nThe analysis filters to DOIs indexed by all three sources\nFor each source, the corresponding topic classification schema is used to generate a count of how many DOIs are linked to each topic\nThe word clouds visualize the top 100 most frequent topics assigned by each source to those shared DOIs\n\nSource-specific classification methods include:\n\nScopus: Author keywords and ASJC codes\nOpenAlex: Topic field from OpenAlex’s hierarchical ontology\nDimensions: Concepts assigned using machine learning (per Dimensions API codebook)\n\nA separate frequency table was generated for each source and dataset combination. These topic counts form the basis of the word clouds shown below.\n\nAgricultural Resource Management Survey (ARMS)\nThese word clouds illustrate the most frequent research topics associated with shared DOIs (N = 30) across Scopus, OpenAlex, and Dimensions for the Agricultural Resource Management Survey (ARMS). While all three sources reflect a core emphasis on agricultural production and economics, the specific framing and vocabulary vary by platform:\nDimensions highlights terms grounded in applied research and policy-oriented topics such as marketing channels, soil health, farm succession, and cash transfers.\nOpenAlex emphasizes conceptual and policy themes like agricultural innovations and practices, organic food and agriculture, and economic and environmental valuation.\nScopus features broader disciplinary categories and methodological terms including economics and econometrics, biofuel, development food science, and soil science.\nThe variation in topical emphasis reflects platform-specific differences in indexing practices, subject classification systems, and coverage of applied versus theoretical scholarship.\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe next set of word clouds summarizes the most frequent research topics associated with publications that reference a given dataset, based on each source’s topic classification schema. The first word cloud in each section aggregates topics across all sources—Scopus, OpenAlex, and Dimensions—to provide a composite view of the research landscape. Readers can then click on source-specific word clouds, which reflect the full corpus of DOIs referencing the dataset within each source. These differences highlight how each platform categorizes scholarly content and may inform decisions about dataset visibility and disciplinary reach.\n\n\n\n\n\n\nAdditional Word Cloud Variants\n\n\n\n\n\n\n  \n\n\nTopics by Source\n\n 🔍 Scopus \n 🔍 OpenAlex \n 🔍 Dimensions \n\n\n\n\n\n\n\nThe Census of Agriculture\nThe word clouds below visualize the most frequent topics assigned to the 210 publications referencing the Census of Agriculture that are indexed in Scopus, OpenAlex, and Dimensions. The first image aggregates topic terms across all three sources. The remaining word clouds show how each individual source categorizes the same set of publications.\nEach classification system presents a different view of the research landscape:\nDimensions emphasizes applied agricultural practice and land management topics, such as Cover Crops, Food Systems, Land Use, and No-Till. Many of the terms reflect production methods, conservation, and on-farm activities.\nOpenAlex includes a wider range of thematic areas, with terms related to sustainability, valuation, and interdisciplinary research. Examples include Urban Agriculture and Sustainability, Economic and Environmental Valuation, and Food Waste Reduction.\nScopus reflects more traditional disciplinary structures, with emphasis on Ecology, Food Science, Economics and Econometrics, and Soil Emission. The presence of terms like China and Urban Agriculture points to geographic and policy framing as well.\nThese differences reflect variation in how each source structures and assigns topical metadata to the same publications.\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe next set of word clouds summarizes the most frequent research topics associated with publications that reference a given dataset, based on each source’s topic classification schema. The first word cloud in each section aggregates topics across all sources—Scopus, OpenAlex, and Dimensions—to provide a composite view of the research landscape. Readers can then click on source-specific word clouds, which reflect the full corpus of DOIs referencing the dataset within each source. These differences highlight how each platform categorizes scholarly content and may inform decisions about dataset visibility and disciplinary reach.\n\n\n\n\n\n\nAdditional Word Cloud Variants\n\n\n\n\n\n\n  \n\n\nTopics by Source\n\n 🔍 Scopus \n 🔍 OpenAlex \n 🔍 Dimensions \n\n\n\n\n\n\n\nFood Access Research Atlas\nFor the 65 publications indexed across Scopus, OpenAlex, and Dimensions that reference the Food Access Research Atlas, topic classifications vary by source. Each database reflects different emphases in how it organizes subject matter related to food environments and neighborhood-level access.\nDimensions highlights terms associated with food insecurity and nutrition assistance, including Food Deserts, Food Insecurity, SNAP Participants, and Census Tracts. The topics are often grounded in program participation, geographic mapping, and diet-related outcomes, suggesting an applied framing centered on public policy and access programs.\nOpenAlex points to broader social and environmental determinants of health, with topics like Obesity, Physical Activity, Diet, Food Security and Health in Diverse Populations, and Urban Agriculture and Sustainability. Its classifications suggest greater integration of population health, urban studies, and structural considerations.\nScopus displays a mix of disciplinary and clinical topics, including Obesity, Grocery Stores, Farmers’ Markets, and Public Health. Additional terms such as Anthropology, Exercise, Surgery, and Biomedical Engineering reflect coverage from journals in the health sciences, indicating a more biomedical orientation.\nTogether, these differences suggest that Dimensions frames FARA-related research through the lens of policy and programmatic access, OpenAlex places greater emphasis on social context and urban health, and Scopus reflects disciplinary classifications from medicine, biology, and public health. These variations may influence how different audiences encounter and interpret research using this dataset.\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe next set of word clouds summarizes the most frequent research topics associated with publications that reference a given dataset, based on each source’s topic classification schema. The first word cloud in each section aggregates topics across all sources—Scopus, OpenAlex, and Dimensions—to provide a composite view of the research landscape. Readers can then click on source-specific word clouds, which reflect the full corpus of DOIs referencing the dataset within each source. These differences highlight how each platform categorizes scholarly content and may inform decisions about dataset visibility and disciplinary reach.\n\n\n\n\n\n\nAdditional Word Cloud Variants\n\n\n\n\n\n\n  \n\n\nTopics by Source\n\n 🔍 Scopus \n 🔍 OpenAlex \n 🔍 Dimensions \n\n\n\n\n\n\n\nThe Food Acquisition and Purchase Survey (FoodAPS)\nAmong the 38 DOIs referencing FoodAPS that appear in all three citation databases, each source assigns different topic labels, offering varied perspectives on the dataset’s use in scholarly research.\nDimensions emphasizes topics directly connected to food purchasing and economic access. Prominent terms include Diet Cost, Food Environment, Thrifty Food Plan, and Supplemental Nutrition Assistance Program. These labels reflect applied work related to food affordability, policy programs, and nutrition behavior, often at the household level.\nOpenAlex highlights broader public health themes such as Obesity, Physical Activity, Diet and Food Security and Health in Diverse Populations. It also includes terms related to structural and behavioral contexts—Urban Agriculture and Sustainability, Homelessness and Social Issues, and Consumer Attitudes and Food Labeling—suggesting a focus on population-level outcomes and intersectional influences on food access.\nScopus shows more disciplinary and intervention-related topics. Terms like Grocery Stores, Farmers’ Markets, Nutrition and Dietetics, and Obesity appear frequently, alongside topics such as Brand Placement, Food Labeling, and Program Participation, indicating interest in behavioral nutrition, food marketing, and policy evaluation.\nThese differences suggest that Dimensions favors classification by programmatic and economic relevance, OpenAlex aligns more with public health and social research, and Scopus tends to organize around disciplinary domains and evaluation studies.\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe next set of word clouds summarizes the most frequent research topics associated with publications that reference a given dataset, based on each source’s topic classification schema. The first word cloud in each section aggregates topics across all sources—Scopus, OpenAlex, and Dimensions—to provide a composite view of the research landscape. Readers can then click on source-specific word clouds, which reflect the full corpus of DOIs referencing the dataset within each source. These differences highlight how each platform categorizes scholarly content and may inform decisions about dataset visibility and disciplinary reach.\n\n\n\n\n\n\nAdditional Word Cloud Variants\n\n\n\n\n\n\n  \n\n\nTopics by Source\n\n 🔍 Scopus \n 🔍 OpenAlex \n 🔍 Dimensions \n\n\n\n\n\n\n\nThe Household Food Security Survey Module\nAmong the 82 DOIs referencing the Household Food Security Survey Module (HFSSM) that are indexed in Scopus, OpenAlex, and Dimensions, each citation database reflects a distinct emphasis in topical classification.\nDimensions highlights food insecurity, supplemental nutrition assistance, and diet quality as central themes, along with demographic and health-related topics such as older adults, physical activity, mental health, and household income. These reflect a strong policy and program-oriented lens, focused on vulnerable populations and health outcomes.\nOpenAlex surfaces broader population health and structural themes. Top topics include Food Security and Health in Diverse Populations, Obesity, Physical Activity, Diet, and Homelessness and Social Issues. The emphasis here leans toward sociomedical framing and public health determinants, especially at the community or systems level.\nScopus features terms like Food Pantries, Program Participation, and Family Characteristic, consistent with food access research. But it also brings in more disciplinary and biomedical language—Epigenetics, Cancer, Autism, and Mindfulness—pointing to research that draws on HFSSM data to explore clinical and psychological outcomes.\nTogether, these patterns show how different databases frame the same set of publications through different classification systems. While the core themes of food security and health are shared, each source emphasizes different disciplinary, policy, or structural dimensions of the research.\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe next set of word clouds summarizes the most frequent research topics associated with publications that reference a given dataset, based on each source’s topic classification schema. The first word cloud in each section aggregates topics across all sources—Scopus, OpenAlex, and Dimensions—to provide a composite view of the research landscape. Readers can then click on source-specific word clouds, which reflect the full corpus of DOIs referencing the dataset within each source. These differences highlight how each platform categorizes scholarly content and may inform decisions about dataset visibility and disciplinary reach.\n\n\n\n\n\n\nAdditional Word Cloud Variants\n\n\n\n\n\n\n  \n\n\nTopics by Source\n\n 🔍 Scopus \n 🔍 OpenAlex \n 🔍 Dimensions \n\n\n\n\n\n\n\nRural-Urban Continuum Code\nAmong the 130 DOIs referencing the Rural-Urban Continuum Code (RUCC) dataset that appear in Scopus, OpenAlex, and Dimensions, topic classifications consistently focus on rural health disparities, healthcare access, and population-level outcomes, though each database frames these themes differently.\nDimensions places the strongest emphasis on county-level characteristics and rural infrastructure. Terms such as rural counties, older adults, cancer incidence, and opioid use disorder are prominent, reflecting the dataset’s utility for examining geographic variation in health outcomes and healthcare delivery.\nOpenAlex centers its taxonomy on population health and structural factors. Topics like opioid use disorder treatment, health disparities and outcomes, and global cancer incidence and screening signal a focus on equity and large-scale health systems research. Behavioral health and environmental health are also prominent themes.\nScopus reflects a broader mix of clinical and disciplinary topics. Frequent terms include Covid-19, cancer, public health, obesity, and health policy. Additional tags such as smoking ban, rural poverty, and Medicaid point to both policy-oriented and biomedical lines of research.\nTogether, these differences illustrate how the same publications are categorized through different topical lenses, depending on the underlying classification systems used by each database.\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe next set of word clouds summarizes the most frequent research topics associated with publications that reference a given dataset, based on each source’s topic classification schema. The first word cloud in each section aggregates topics across all sources—Scopus, OpenAlex, and Dimensions—to provide a composite view of the research landscape. Readers can then click on source-specific word clouds, which reflect the full corpus of DOIs referencing the dataset within each source. These differences highlight how each platform categorizes scholarly content and may inform decisions about dataset visibility and disciplinary reach.\n\n\n\n\n\n\nAdditional Word Cloud Variants\n\n\n\n\n\n\n  \n\n\nTopics by Source\n\n 🔍 Scopus \n 🔍 OpenAlex \n 🔍 Dimensions \n\n\n\n\n\n\n\n\n2.3.4 Author Comparison\nTo identify and compare authors across Scopus, OpenAlex, and Dimensions, a multi-step disambiguation process was implemented. Because not all authors have persistent identifiers (e.g., ORCIDs), and because name formatting, use of initials, and institutional affiliations vary across and within sources, a harmonization pipeline was developed. This process follows the structure of the PatentsView disambiguation methodology and includes the following steps:\n\nName Normalization and Source-Specific Cleaning: Author names were extracted from each source and cleaned using a consistent normalization function. This involved transliterating special characters, removing punctuation, standardizing case, and collapsing whitespace. In each database, author records were linked to publication DOIs and enriched with affiliation information where available.\nORCID-Based Canonical Resolution: When an author’s ORCID was present—either directly in OpenAlex or indirectly via Dimensions—it was used as the canonical identifier. ORCID lookups were performed for all DOIs across sources, and a lookup table was constructed to resolve shared authors using both ORCID and cleaned name/DOI matches.\nBlocking Using Canopy Construction: For authors without ORCID identifiers, blocking keys were constructed by combining the first initial and last name to form “canopy” groups. This reduced the number of pairwise comparisons needed for clustering by limiting them to plausible matches.\nString Similarity Clustering Within Canopies: Within each canopy group, Jaro-Winkler string distances were calculated using the cleaned full names. Hierarchical clustering with average linkage was applied, and clusters were formed using a similarity threshold. Each cluster was then assigned a synthetic canonical ID based on the first observed name.\nMerging and Source Propagation: Author mentions across all three sources were merged into a master long-format table, with canonical IDs assigned based on ORCID or string-based clustering. For each publication, flags were added to indicate whether an author appeared in Scopus, OpenAlex, or Dimensions. These flags were propagated to all mentions of a given author within the same DOI.\nInstitutional Consolidation: Author affiliations were collapsed across sources by pivoting to a wide format (institution_1, institution_2, etc.) and summarizing into a primary institution field. This structure supported subsequent author-level aggregation and topic classification.\n\nThis approach enables the identification of unique authors across bibliometric systems, even in the absence of persistent identifiers. It supports comparisons of author counts, top contributors, and topic-specific participation across Scopus, OpenAlex, and Dimensions.\nMain Takeaway\nAcross all datasets, the authors most visible in one platform are not always discoverable in others. The top contributors to a dataset can vary significantly depending on which citation database is used. These differences stem from inconsistencies in metadata, name disambiguation, and indexing practices. As a result, evaluations or dashboards based on a single source may misrepresent who is using a dataset, leading to undercounting or omission of active researchers. Using multiple sources helps create a more accurate and equitable picture of scholarly engagement.\n\n\n2.3.5 Author Mentions by Dataset and Citation Database (2017–2023, Article-Type Only)\n\n\n\n\n\n\n\n\n\nUSDA Dataset\nUnique Authors (Scopus)\nUnique Authors (OpenAlex)\nUnique Authors (Dimensions)\n\n\n\n\nAgricultural Resource Management Survey (ARMS)\n732\n4,464\n648\n\n\nCensus of Agriculture\n13,373\n4,727\n9,320\n\n\nFood Access Research Atlas (FARA)\n1,601\n1,153\n13,588\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\n1,133\n1,849\n846\n\n\nHousehold Food Security Survey Module (HFSSM)\n3,536\n1,661\n2,520\n\n\nRural-Urban Continuum Code (RUCC)\n6,894\n1,882\n6,478\n\n\n\nNote: Author counts reflect unique individuals associated with article-type publications from 2017 to 2023 that mention the specified USDA dataset in Scopus, OpenAlex, or Dimensions. Each citation database represents its own independently derived corpus, with publication inclusion determined by dataset keyword searches and metadata filters.\nThe full author-level summary by dataset and citation source is available here.\nFor purposes of this report, we feature the top 20 authors by publication count for each dataset and source. These visualizations help illustrate how platform-specific indexing affects which researchers appear most prominently. Comparing leading authors across Scopus, OpenAlex, and Dimensions reveals potential disparities in dataset visibility and discoverability.\n\nARMSCensus of AgFARAFoodAPSHFSSMRUCC\n\n\nFor ARMS, the top 20 authors identified in each platform show limited overlap. While some authors are discoverable across all three sources, others appear only in one, particularly in OpenAlex or Dimensions. This reflects inconsistencies in how author names are indexed and matched across platforms, especially for researchers who publish under multiple name variants or without ORCID identifiers.\n\n  \n\n\n\nThe Census of Agriculture shows relatively higher agreement across platforms, with many top authors appearing in multiple sources. However, there are still noticeable differences, with some authors ranked highly in one platform but not appearing at all in others. These discrepancies are likely tied to variations in how author metadata and institutional affiliations are recorded.\n\n  \n\n\n\nFARA displays substantial divergence in author coverage. A number of top authors are visible in only one platform, and overlap across all three is relatively limited. This dataset seems particularly affected by platform-specific indexing practices—likely because much of the associated research is interdisciplinary and published across a range of journal types.\n\n  \n\n\n\nFoodAPS has uneven author coverage across platforms. While some authors are picked up consistently, others are captured by only one source. OpenAlex includes several authors who are not visible in Scopus or Dimensions, suggesting that coverage differences may be especially pronounced for newer researchers or those publishing in open-access venues.\n\n  \n\n\n\nThe HFSSM dataset exhibits moderate agreement in author coverage. Most top authors are represented in at least two sources, but each platform still identifies several authors not seen in the others. This suggests that while the dataset has relatively broad exposure, gaps remain that could affect who is counted or highlighted in bibliometric analyses.\n\n  \n\n\n\nRUCC shows the widest variation in author rankings. Many authors appear in only one of the three sources, and very few are consistently represented across all. This fragmentation likely reflects the broad disciplinary scope of RUCC-related research, which spans public health, demography, and social science—fields that are not uniformly indexed across platforms.\n\n  \n\n\n\n\n\n\n2.3.6 Institutional Comparison\nIn addition to examining dataset mention coverage, the report also evaluates differences in institutional representation across Scopus, OpenAlex, and Dimensions. Each of the featured citation databases represent some portion of the global research landscape, yet their inclusion criteria and institutional coverage may vary. The purpose of this analysis is to assess which institutions are represented in each source.\n\nScopusDimensionsOpenAlex",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#conclusion",
    "href": "report.html#conclusion",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.4 Conclusion",
    "text": "2.4 Conclusion\nThis report compares how publications referencing the Census of Agriculture are captured across Scopus, OpenAlex, and Dimensions. While each platform provides partial visibility into dataset usage, the overlap in coverage remains limited. Differences in indexing, topic classification, and full-text availability lead to variation in which publications, topics, and authors are discoverable. These findings highlight how the choice of citation platform can shape perceptions of research influence and dataset reach.",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "report.html#footnotes",
    "href": "report.html#footnotes",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA dataset mention refers to an instance in which a specific dataset is referenced, cited, or named within a research publication. This can occur in various parts of the text, such as the abstract, methods, data section, footnotes, or references, and typically indicates that the dataset was used, analyzed, or discussed in the study.↩︎\nFull-text search in OpenAlex refers to querying the entire database for textual mentions of dataset names within titles, abstracts, and other fields.↩︎\nThe seed corpus search involves selecting a targeted set of publications based on journal, author, and topic filters. Full-text PDFs are downloaded and analyzed to identify mentions of USDA datasets not captured through metadata alone.↩︎\nThis procedure increased the likelihood of capturing genuine dataset references rather than incidental matches to individual words. Initial drafts of the query incorrectly included terms like “NASS” and “USDA” in the alias list. This was corrected to ensure that aliases strictly referred to dataset names, and flag terms referred to organizations.↩︎\nPyalex is an open-source library designed to facilitate interaction with the OpenAlex API; see https://help.openalex.org/hc/en-us/articles/27086501974551-Projects-Using-OpenAlex for more information. The package manages request formatting and automates compliance with OpenAlex’s “polite pool” rate limits, which restrict the number of requests per minute and impose backoff delays. Pyalex introduced automatic pauses between requests, with a default retry_backoff_factor of 100 milliseconds, to ensure stable and continuous retrieval. This setup enabled systematic querying while adhering to OpenAlex’s usage policies.↩︎\nIn cases where a publication appeared in more than one source, manual and programmatic checks confirmed that metadata values, such as journal titles and publication years, were consistent across sources. No conflicting values were detected.↩︎",
    "crumbs": [
      "Full Report"
    ]
  },
  {
    "objectID": "webinar_questionnaire.html",
    "href": "webinar_questionnaire.html",
    "title": "Questionnaire",
    "section": "",
    "text": "Complete Questionnaire \n\nLoading…\n\n\n\n\n\nInterested in staying involved? If you’d like to join a working group to continue the conversation and help shape next steps, please provide your contact information. Participation is completely optional.\n\n Sign Up Here \n\nLoading…",
    "crumbs": [
      "Questionnaire"
    ]
  },
  {
    "objectID": "code/flatten_openalex_all_datasets.html",
    "href": "code/flatten_openalex_all_datasets.html",
    "title": "Looped over all datasets",
    "section": "",
    "text": "Use the NASS case to create a loop across all json’s in the folder:\nOutput: - A dataset_reference_table.csv file with all your filenames and IDs - A folder for each dataset: - flattened_dataset01/ - dataset01_main.csv - dataset01_authorships.csv … - flattened_dataset02/ - dataset02_main.csv …\nEverything is neatly contained and reproducible.\n# Step 1: List and tag all JSON files\nimport os\nimport pandas as pd\n\n# Set the folder where the JSON files live\ndata_dir = \".\"  # or specify full path if needed\n\n# List all .json files\njson_files = [f for f in os.listdir(data_dir) if f.endswith(\".json\")]\njson_files.sort()  # ensure consistent ordering\n\n# Create IDs like dataset01, dataset02...\ndataset_ids = [f\"dataset{str(i+1).zfill(2)}\" for i in range(len(json_files))]\n\n# Create reference table\nreference_table = pd.DataFrame({\n    \"dataset_id\": dataset_ids,\n    \"filename\": json_files\n})\n\n# Save reference table\nreference_table.to_csv(\"dataset_reference_table.csv\", index=False)\nprint(reference_table)\n\n   dataset_id                                           filename\n0   dataset01                                          ARMS.json\n1   dataset02  Current Population Survey Food Security Supple...\n2   dataset03                         Farm to School Census.json\n3   dataset04    Information Resources, Inc. (IRI) InfoScan.json\n4   dataset05                    NASS Census of Agriculture.json\n5   dataset06                                          RUCC.json\n6   dataset07  Tenure, ownership, and transition of agricultu...\n7   dataset08                    food access research atlas.json\n8   dataset09          food acquisition and purchase survey.json\n9   dataset10         household food security survey module.json\n10  dataset11         local food marketing practices survey.json\n11  dataset12         quarterly food at home price database.json\n12  dataset13        transition of agricultural land survey.json\n# Step 2: Loop through each JSON and apply flattening\n\n# Define fields to exclude from flattening\nexclude_fields = [\n    \"keywords\",\n    \"sustainable_development_goals\",\n    \"mesh\",\n    \"referenced_works\",\n    \"related_works\",\n    \"abstract_inverted_index\",\n    \"plain_text\",\n    \"concepts\",\n    \"locations\"\n]\n\n# Check if a value is a nested structure\ndef is_nested(val):\n    return isinstance(val, (list, dict))\n\ndef deep_flatten_field(data, field, id_key=\"id\"):\n    rows = []\n    for rec in data:\n        parent_id = rec.get(id_key)\n        items = rec.get(field, [])\n\n        if isinstance(items, dict):\n            flat = flatten(items)\n            flat['parent_id'] = parent_id\n            rows.append(flat)\n        elif isinstance(items, list):\n            for item in items:\n                if isinstance(item, dict):\n                    flat = flatten(item)\n                    flat['parent_id'] = parent_id\n                    rows.append(flat)\n                else:\n                    rows.append({'value': item, 'parent_id': parent_id})\n\n    return pd.DataFrame(rows) if rows else pd.DataFrame()\n# Step 3: Process all datasets\n\nimport json\nfrom flatten_json import flatten\n\nimport csv\n\nfor i, row in reference_table.iterrows():\n    dataset_id = row[\"dataset_id\"]\n    file_name = row[\"filename\"]\n    print(f\"\\n Processing {file_name} → {dataset_id}\")\n\n    # Load JSON\n    with open(os.path.join(data_dir, file_name), \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    # Flatten top level\n    df_main = pd.DataFrame([flatten(record) for record in data])\n\n    # Save main flat table\n    output_dir = f\"flattened_{dataset_id}\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # ---- Clean any old CSV files ----\n    for f in os.listdir(output_dir):\n        if f.endswith(\".csv\"):\n            os.remove(os.path.join(output_dir, f))\n    \n    main_csv_path = os.path.join(output_dir, f\"{dataset_id}_main.csv\")\n    df_main.to_csv(main_csv_path, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n\n    # Validate saved main CSV\n    try:\n        # Suppress DtypeWarning by adding low_memory=False\n        _ = pd.read_csv(main_csv_path, low_memory=False)\n        print(f\"Validated: {main_csv_path}\")\n    except Exception as e:\n        print(f\"Validation error in {main_csv_path}: {e}\")\n\n    # Identify nested fields\n    nested_fields = set()\n    for rec in data:\n        for k, v in rec.items():\n            if k not in exclude_fields and is_nested(v):\n                nested_fields.add(k)\n\n    # Flatten each nested field\n    for field in nested_fields:\n        df_nested = deep_flatten_field(data, field)\n        if not df_nested.empty:\n            nested_csv_path = os.path.join(output_dir, f\"{dataset_id}_{field}.csv\")\n            df_nested.to_csv(nested_csv_path, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n\n            # Validate nested saved CSV\n            try:\n                # Suppress DtypeWarning here too\n                _ = pd.read_csv(nested_csv_path, low_memory=False)\n                print(f\"Validated: {nested_csv_path}\")\n            except Exception as e:\n                print(f\"Validation error in {nested_csv_path}: {e}\")\n\n    print(f\"Finished processing {dataset_id} — saved to {output_dir}\")\n\n\n Processing ARMS.json → dataset01\nValidated: flattened_dataset01\\dataset01_main.csv\nValidated: flattened_dataset01\\dataset01_corresponding_author_ids.csv\nValidated: flattened_dataset01\\dataset01_grants.csv\nValidated: flattened_dataset01\\dataset01_apc_list.csv\nValidated: flattened_dataset01\\dataset01_indexed_in.csv\nValidated: flattened_dataset01\\dataset01_primary_topic.csv\nValidated: flattened_dataset01\\dataset01__id.csv\nValidated: flattened_dataset01\\dataset01_counts_by_year.csv\nValidated: flattened_dataset01\\dataset01_citation_normalized_percentile.csv\nValidated: flattened_dataset01\\dataset01_cited_by_percentile_year.csv\nValidated: flattened_dataset01\\dataset01_primary_location.csv\nValidated: flattened_dataset01\\dataset01_apc_paid.csv\nValidated: flattened_dataset01\\dataset01_topics.csv\nValidated: flattened_dataset01\\dataset01_dataset.csv\nValidated: flattened_dataset01\\dataset01_best_oa_location.csv\nValidated: flattened_dataset01\\dataset01_ids.csv\nValidated: flattened_dataset01\\dataset01_authorships.csv\nValidated: flattened_dataset01\\dataset01_datasets.csv\nValidated: flattened_dataset01\\dataset01_open_access.csv\nValidated: flattened_dataset01\\dataset01_corresponding_institution_ids.csv\nValidated: flattened_dataset01\\dataset01_biblio.csv\nFinished processing dataset01 — saved to flattened_dataset01\n\n Processing Current Population Survey Food Security Supplement.json → dataset02\nValidated: flattened_dataset02\\dataset02_main.csv\nValidated: flattened_dataset02\\dataset02_corresponding_author_ids.csv\nValidated: flattened_dataset02\\dataset02_grants.csv\nValidated: flattened_dataset02\\dataset02_apc_list.csv\nValidated: flattened_dataset02\\dataset02_indexed_in.csv\nValidated: flattened_dataset02\\dataset02_primary_topic.csv\nValidated: flattened_dataset02\\dataset02__id.csv\nValidated: flattened_dataset02\\dataset02_counts_by_year.csv\nValidated: flattened_dataset02\\dataset02_citation_normalized_percentile.csv\nValidated: flattened_dataset02\\dataset02_cited_by_percentile_year.csv\nValidated: flattened_dataset02\\dataset02_primary_location.csv\nValidated: flattened_dataset02\\dataset02_apc_paid.csv\nValidated: flattened_dataset02\\dataset02_topics.csv\nValidated: flattened_dataset02\\dataset02_dataset.csv\nValidated: flattened_dataset02\\dataset02_best_oa_location.csv\nValidated: flattened_dataset02\\dataset02_ids.csv\nValidated: flattened_dataset02\\dataset02_authorships.csv\nValidated: flattened_dataset02\\dataset02_datasets.csv\nValidated: flattened_dataset02\\dataset02_open_access.csv\nValidated: flattened_dataset02\\dataset02_corresponding_institution_ids.csv\nValidated: flattened_dataset02\\dataset02_biblio.csv\nFinished processing dataset02 — saved to flattened_dataset02\n\n Processing Farm to School Census.json → dataset03\nValidated: flattened_dataset03\\dataset03_main.csv\nValidated: flattened_dataset03\\dataset03_corresponding_author_ids.csv\nValidated: flattened_dataset03\\dataset03_grants.csv\nValidated: flattened_dataset03\\dataset03_apc_list.csv\nValidated: flattened_dataset03\\dataset03_indexed_in.csv\nValidated: flattened_dataset03\\dataset03_primary_topic.csv\nValidated: flattened_dataset03\\dataset03__id.csv\nValidated: flattened_dataset03\\dataset03_counts_by_year.csv\nValidated: flattened_dataset03\\dataset03_citation_normalized_percentile.csv\nValidated: flattened_dataset03\\dataset03_cited_by_percentile_year.csv\nValidated: flattened_dataset03\\dataset03_primary_location.csv\nValidated: flattened_dataset03\\dataset03_apc_paid.csv\nValidated: flattened_dataset03\\dataset03_topics.csv\nValidated: flattened_dataset03\\dataset03_dataset.csv\nValidated: flattened_dataset03\\dataset03_best_oa_location.csv\nValidated: flattened_dataset03\\dataset03_ids.csv\nValidated: flattened_dataset03\\dataset03_authorships.csv\nValidated: flattened_dataset03\\dataset03_open_access.csv\nValidated: flattened_dataset03\\dataset03_corresponding_institution_ids.csv\nValidated: flattened_dataset03\\dataset03_biblio.csv\nFinished processing dataset03 — saved to flattened_dataset03\n\n Processing Information Resources, Inc. (IRI) InfoScan.json → dataset04\nValidated: flattened_dataset04\\dataset04_main.csv\nValidated: flattened_dataset04\\dataset04_corresponding_author_ids.csv\nValidated: flattened_dataset04\\dataset04_grants.csv\nValidated: flattened_dataset04\\dataset04_apc_list.csv\nValidated: flattened_dataset04\\dataset04_indexed_in.csv\nValidated: flattened_dataset04\\dataset04_primary_topic.csv\nValidated: flattened_dataset04\\dataset04__id.csv\nValidated: flattened_dataset04\\dataset04_counts_by_year.csv\nValidated: flattened_dataset04\\dataset04_citation_normalized_percentile.csv\nValidated: flattened_dataset04\\dataset04_cited_by_percentile_year.csv\nValidated: flattened_dataset04\\dataset04_primary_location.csv\nValidated: flattened_dataset04\\dataset04_apc_paid.csv\nValidated: flattened_dataset04\\dataset04_topics.csv\nValidated: flattened_dataset04\\dataset04_dataset.csv\nValidated: flattened_dataset04\\dataset04_best_oa_location.csv\nValidated: flattened_dataset04\\dataset04_ids.csv\nValidated: flattened_dataset04\\dataset04_authorships.csv\nValidated: flattened_dataset04\\dataset04_open_access.csv\nValidated: flattened_dataset04\\dataset04_corresponding_institution_ids.csv\nValidated: flattened_dataset04\\dataset04_biblio.csv\nFinished processing dataset04 — saved to flattened_dataset04\n\n Processing NASS Census of Agriculture.json → dataset05\nValidated: flattened_dataset05\\dataset05_main.csv\nValidated: flattened_dataset05\\dataset05_corresponding_author_ids.csv\nValidated: flattened_dataset05\\dataset05_grants.csv\nValidated: flattened_dataset05\\dataset05_apc_list.csv\nValidated: flattened_dataset05\\dataset05_indexed_in.csv\nValidated: flattened_dataset05\\dataset05_primary_topic.csv\nValidated: flattened_dataset05\\dataset05__id.csv\nValidated: flattened_dataset05\\dataset05_counts_by_year.csv\nValidated: flattened_dataset05\\dataset05_citation_normalized_percentile.csv\nValidated: flattened_dataset05\\dataset05_cited_by_percentile_year.csv\nValidated: flattened_dataset05\\dataset05_primary_location.csv\nValidated: flattened_dataset05\\dataset05_apc_paid.csv\nValidated: flattened_dataset05\\dataset05_topics.csv\nValidated: flattened_dataset05\\dataset05_dataset.csv\nValidated: flattened_dataset05\\dataset05_best_oa_location.csv\nValidated: flattened_dataset05\\dataset05_ids.csv\nValidated: flattened_dataset05\\dataset05_versions.csv\nValidated: flattened_dataset05\\dataset05_authorships.csv\nValidated: flattened_dataset05\\dataset05_datasets.csv\nValidated: flattened_dataset05\\dataset05_open_access.csv\nValidated: flattened_dataset05\\dataset05_corresponding_institution_ids.csv\nValidated: flattened_dataset05\\dataset05_biblio.csv\nFinished processing dataset05 — saved to flattened_dataset05\n\n Processing RUCC.json → dataset06\nValidated: flattened_dataset06\\dataset06_main.csv\nValidated: flattened_dataset06\\dataset06_corresponding_author_ids.csv\nValidated: flattened_dataset06\\dataset06_grants.csv\nValidated: flattened_dataset06\\dataset06_apc_list.csv\nValidated: flattened_dataset06\\dataset06_indexed_in.csv\nValidated: flattened_dataset06\\dataset06_primary_topic.csv\nValidated: flattened_dataset06\\dataset06__id.csv\nValidated: flattened_dataset06\\dataset06_counts_by_year.csv\nValidated: flattened_dataset06\\dataset06_citation_normalized_percentile.csv\nValidated: flattened_dataset06\\dataset06_cited_by_percentile_year.csv\nValidated: flattened_dataset06\\dataset06_primary_location.csv\nValidated: flattened_dataset06\\dataset06_apc_paid.csv\nValidated: flattened_dataset06\\dataset06_topics.csv\nValidated: flattened_dataset06\\dataset06_dataset.csv\nValidated: flattened_dataset06\\dataset06_best_oa_location.csv\nValidated: flattened_dataset06\\dataset06_ids.csv\nValidated: flattened_dataset06\\dataset06_authorships.csv\nValidated: flattened_dataset06\\dataset06_open_access.csv\nValidated: flattened_dataset06\\dataset06_corresponding_institution_ids.csv\nValidated: flattened_dataset06\\dataset06_biblio.csv\nFinished processing dataset06 — saved to flattened_dataset06\n\n Processing Tenure, ownership, and transition of agricultural land.json → dataset07\nValidated: flattened_dataset07\\dataset07_main.csv\nValidated: flattened_dataset07\\dataset07_corresponding_author_ids.csv\nValidated: flattened_dataset07\\dataset07_grants.csv\nValidated: flattened_dataset07\\dataset07_apc_list.csv\nValidated: flattened_dataset07\\dataset07_indexed_in.csv\nValidated: flattened_dataset07\\dataset07_primary_topic.csv\nValidated: flattened_dataset07\\dataset07__id.csv\nValidated: flattened_dataset07\\dataset07_counts_by_year.csv\nValidated: flattened_dataset07\\dataset07_citation_normalized_percentile.csv\nValidated: flattened_dataset07\\dataset07_cited_by_percentile_year.csv\nValidated: flattened_dataset07\\dataset07_primary_location.csv\nValidated: flattened_dataset07\\dataset07_apc_paid.csv\nValidated: flattened_dataset07\\dataset07_topics.csv\nValidated: flattened_dataset07\\dataset07_dataset.csv\nValidated: flattened_dataset07\\dataset07_best_oa_location.csv\nValidated: flattened_dataset07\\dataset07_ids.csv\nValidated: flattened_dataset07\\dataset07_authorships.csv\nValidated: flattened_dataset07\\dataset07_open_access.csv\nValidated: flattened_dataset07\\dataset07_corresponding_institution_ids.csv\nValidated: flattened_dataset07\\dataset07_biblio.csv\nFinished processing dataset07 — saved to flattened_dataset07\n\n Processing food access research atlas.json → dataset08\nValidated: flattened_dataset08\\dataset08_main.csv\nValidated: flattened_dataset08\\dataset08_corresponding_author_ids.csv\nValidated: flattened_dataset08\\dataset08_grants.csv\nValidated: flattened_dataset08\\dataset08_apc_list.csv\nValidated: flattened_dataset08\\dataset08_indexed_in.csv\nValidated: flattened_dataset08\\dataset08_primary_topic.csv\nValidated: flattened_dataset08\\dataset08__id.csv\nValidated: flattened_dataset08\\dataset08_counts_by_year.csv\nValidated: flattened_dataset08\\dataset08_citation_normalized_percentile.csv\nValidated: flattened_dataset08\\dataset08_cited_by_percentile_year.csv\nValidated: flattened_dataset08\\dataset08_primary_location.csv\nValidated: flattened_dataset08\\dataset08_apc_paid.csv\nValidated: flattened_dataset08\\dataset08_topics.csv\nValidated: flattened_dataset08\\dataset08_dataset.csv\nValidated: flattened_dataset08\\dataset08_best_oa_location.csv\nValidated: flattened_dataset08\\dataset08_ids.csv\nValidated: flattened_dataset08\\dataset08_authorships.csv\nValidated: flattened_dataset08\\dataset08_open_access.csv\nValidated: flattened_dataset08\\dataset08_corresponding_institution_ids.csv\nValidated: flattened_dataset08\\dataset08_biblio.csv\nFinished processing dataset08 — saved to flattened_dataset08\n\n Processing food acquisition and purchase survey.json → dataset09\nValidated: flattened_dataset09\\dataset09_main.csv\nValidated: flattened_dataset09\\dataset09_corresponding_author_ids.csv\nValidated: flattened_dataset09\\dataset09_grants.csv\nValidated: flattened_dataset09\\dataset09_apc_list.csv\nValidated: flattened_dataset09\\dataset09_indexed_in.csv\nValidated: flattened_dataset09\\dataset09_primary_topic.csv\nValidated: flattened_dataset09\\dataset09__id.csv\nValidated: flattened_dataset09\\dataset09_counts_by_year.csv\nValidated: flattened_dataset09\\dataset09_citation_normalized_percentile.csv\nValidated: flattened_dataset09\\dataset09_cited_by_percentile_year.csv\nValidated: flattened_dataset09\\dataset09_primary_location.csv\nValidated: flattened_dataset09\\dataset09_apc_paid.csv\nValidated: flattened_dataset09\\dataset09_topics.csv\nValidated: flattened_dataset09\\dataset09_dataset.csv\nValidated: flattened_dataset09\\dataset09_best_oa_location.csv\nValidated: flattened_dataset09\\dataset09_ids.csv\nValidated: flattened_dataset09\\dataset09_authorships.csv\nValidated: flattened_dataset09\\dataset09_datasets.csv\nValidated: flattened_dataset09\\dataset09_open_access.csv\nValidated: flattened_dataset09\\dataset09_corresponding_institution_ids.csv\nValidated: flattened_dataset09\\dataset09_biblio.csv\nFinished processing dataset09 — saved to flattened_dataset09\n\n Processing household food security survey module.json → dataset10\nValidated: flattened_dataset10\\dataset10_main.csv\nValidated: flattened_dataset10\\dataset10_corresponding_author_ids.csv\nValidated: flattened_dataset10\\dataset10_grants.csv\nValidated: flattened_dataset10\\dataset10_apc_list.csv\nValidated: flattened_dataset10\\dataset10_indexed_in.csv\nValidated: flattened_dataset10\\dataset10_primary_topic.csv\nValidated: flattened_dataset10\\dataset10__id.csv\nValidated: flattened_dataset10\\dataset10_counts_by_year.csv\nValidated: flattened_dataset10\\dataset10_citation_normalized_percentile.csv\nValidated: flattened_dataset10\\dataset10_cited_by_percentile_year.csv\nValidated: flattened_dataset10\\dataset10_primary_location.csv\nValidated: flattened_dataset10\\dataset10_apc_paid.csv\nValidated: flattened_dataset10\\dataset10_topics.csv\nValidated: flattened_dataset10\\dataset10_dataset.csv\nValidated: flattened_dataset10\\dataset10_best_oa_location.csv\nValidated: flattened_dataset10\\dataset10_ids.csv\nValidated: flattened_dataset10\\dataset10_authorships.csv\nValidated: flattened_dataset10\\dataset10_datasets.csv\nValidated: flattened_dataset10\\dataset10_open_access.csv\nValidated: flattened_dataset10\\dataset10_corresponding_institution_ids.csv\nValidated: flattened_dataset10\\dataset10_biblio.csv\nFinished processing dataset10 — saved to flattened_dataset10\n\n Processing local food marketing practices survey.json → dataset11\nValidated: flattened_dataset11\\dataset11_main.csv\nValidated: flattened_dataset11\\dataset11_corresponding_author_ids.csv\nValidated: flattened_dataset11\\dataset11_grants.csv\nValidated: flattened_dataset11\\dataset11_apc_list.csv\nValidated: flattened_dataset11\\dataset11_indexed_in.csv\nValidated: flattened_dataset11\\dataset11_primary_topic.csv\nValidated: flattened_dataset11\\dataset11__id.csv\nValidated: flattened_dataset11\\dataset11_counts_by_year.csv\nValidated: flattened_dataset11\\dataset11_citation_normalized_percentile.csv\nValidated: flattened_dataset11\\dataset11_cited_by_percentile_year.csv\nValidated: flattened_dataset11\\dataset11_primary_location.csv\nValidated: flattened_dataset11\\dataset11_apc_paid.csv\nValidated: flattened_dataset11\\dataset11_topics.csv\nValidated: flattened_dataset11\\dataset11_dataset.csv\nValidated: flattened_dataset11\\dataset11_best_oa_location.csv\nValidated: flattened_dataset11\\dataset11_ids.csv\nValidated: flattened_dataset11\\dataset11_authorships.csv\nValidated: flattened_dataset11\\dataset11_open_access.csv\nValidated: flattened_dataset11\\dataset11_corresponding_institution_ids.csv\nValidated: flattened_dataset11\\dataset11_biblio.csv\nFinished processing dataset11 — saved to flattened_dataset11\n\n Processing quarterly food at home price database.json → dataset12\nValidated: flattened_dataset12\\dataset12_main.csv\nValidated: flattened_dataset12\\dataset12_corresponding_author_ids.csv\nValidated: flattened_dataset12\\dataset12_grants.csv\nValidated: flattened_dataset12\\dataset12_apc_list.csv\nValidated: flattened_dataset12\\dataset12_indexed_in.csv\nValidated: flattened_dataset12\\dataset12_primary_topic.csv\nValidated: flattened_dataset12\\dataset12__id.csv\nValidated: flattened_dataset12\\dataset12_counts_by_year.csv\nValidated: flattened_dataset12\\dataset12_citation_normalized_percentile.csv\nValidated: flattened_dataset12\\dataset12_cited_by_percentile_year.csv\nValidated: flattened_dataset12\\dataset12_primary_location.csv\nValidated: flattened_dataset12\\dataset12_apc_paid.csv\nValidated: flattened_dataset12\\dataset12_topics.csv\nValidated: flattened_dataset12\\dataset12_dataset.csv\nValidated: flattened_dataset12\\dataset12_best_oa_location.csv\nValidated: flattened_dataset12\\dataset12_ids.csv\nValidated: flattened_dataset12\\dataset12_authorships.csv\nValidated: flattened_dataset12\\dataset12_open_access.csv\nValidated: flattened_dataset12\\dataset12_corresponding_institution_ids.csv\nValidated: flattened_dataset12\\dataset12_biblio.csv\nFinished processing dataset12 — saved to flattened_dataset12\n\n Processing transition of agricultural land survey.json → dataset13\nValidated: flattened_dataset13\\dataset13_main.csv\nValidated: flattened_dataset13\\dataset13_corresponding_author_ids.csv\nValidated: flattened_dataset13\\dataset13_grants.csv\nValidated: flattened_dataset13\\dataset13_apc_list.csv\nValidated: flattened_dataset13\\dataset13_indexed_in.csv\nValidated: flattened_dataset13\\dataset13_primary_topic.csv\nValidated: flattened_dataset13\\dataset13__id.csv\nValidated: flattened_dataset13\\dataset13_counts_by_year.csv\nValidated: flattened_dataset13\\dataset13_citation_normalized_percentile.csv\nValidated: flattened_dataset13\\dataset13_cited_by_percentile_year.csv\nValidated: flattened_dataset13\\dataset13_primary_location.csv\nValidated: flattened_dataset13\\dataset13_apc_paid.csv\nValidated: flattened_dataset13\\dataset13_topics.csv\nValidated: flattened_dataset13\\dataset13_dataset.csv\nValidated: flattened_dataset13\\dataset13_best_oa_location.csv\nValidated: flattened_dataset13\\dataset13_ids.csv\nValidated: flattened_dataset13\\dataset13_authorships.csv\nValidated: flattened_dataset13\\dataset13_open_access.csv\nValidated: flattened_dataset13\\dataset13_corresponding_institution_ids.csv\nValidated: flattened_dataset13\\dataset13_biblio.csv\nFinished processing dataset13 — saved to flattened_dataset13"
  },
  {
    "objectID": "code/flatten_openalex_all_datasets.html#generate-a-schema-diagram",
    "href": "code/flatten_openalex_all_datasets.html#generate-a-schema-diagram",
    "title": "Looped over all datasets",
    "section": "1 Generate a Schema Diagram",
    "text": "1 Generate a Schema Diagram\n\npip install graphviz\n\nRequirement already satisfied: graphviz in c:\\users\\lachenar\\appdata\\local\\anaconda3\\lib\\site-packages (0.20.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport os\nos.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\Graphviz\\bin\"\n\nimport shutil\nprint(shutil.which(\"dot\"))  # Should now return the full path\n\nC:\\Program Files\\Graphviz\\bin\\dot.EXE\n\n\n\nimport pandas as pd\nfrom graphviz import Digraph\n\n# Set directory where flattened outputs are\nbase_dir = \".\"  # or full path\n\n# Get all dataset folders\ndataset_folders = [f for f in os.listdir(base_dir) if f.startswith(\"flattened_dataset\")]\n\n# Loop through each dataset\nfor dataset_folder in dataset_folders:\n    print(f\"Building schema for {dataset_folder}\")\n    dot = Digraph(comment=f\"Schema: {dataset_folder}\", format=\"png\")\n    dot.attr(rankdir=\"LR\")\n\n    csv_files = [f for f in os.listdir(dataset_folder) if f.endswith(\".csv\")]\n\n    tables = {}\n\n    for csv_file in csv_files:\n        table_name = csv_file.replace(\".csv\", \"\")\n        path = os.path.join(dataset_folder, csv_file)\n\n        # Read only first few rows for speed\n        df = pd.read_csv(path, nrows=500)\n\n        # Summarize schema\n        col_lines = []\n        max_cols = 10  # show up to 10 fields\n        for i, col in enumerate(df.columns[:max_cols]):\n            dtype = str(df[col].dtype)\n            col_lines.append(f\"{col} : {dtype}\")\n        if len(df.columns) &gt; max_cols:\n            col_lines.append(\"...\")  # indicate there's more\n\n        table_def = f\"{{{table_name}|{'\\\\l'.join(col_lines)}\\\\l}}\"\n\n        # Store\n        tables[table_name] = {\n            \"cols\": df.columns.tolist(),\n            \"definition\": table_def\n        }\n        dot.node(table_name, label=table_def, shape=\"record\")\n\n    # Draw relationships (parent_id → id)\n    for table_name, info in tables.items():\n        cols = info[\"cols\"]\n        if \"parent_id\" in cols:\n            dot.edge(table_name, f\"{dataset_folder}_main\", label=\"parent_id → id\")\n\n    # Save diagram\n    output_path = os.path.join(dataset_folder, f\"{dataset_folder}_schema\")\n    dot.render(output_path, cleanup=True)\n    print(f\"Saved schema diagram to {output_path}.png\")\n\n\nimport os\nimport pandas as pd\n\n# Path to the dataset folder (e.g., flattened_dataset01)\ndataset_folder = \"flattened_dataset01\"\ndataset_id = \"dataset01\"  # used to build column links\n\nschema_rows = []\n\n# Loop through each CSV file\nfor csv_file in os.listdir(dataset_folder):\n    if csv_file.endswith(\".csv\"):\n        table_name = csv_file.replace(\".csv\", \"\")\n        file_path = os.path.join(dataset_folder, csv_file)\n\n        # Read a sample of rows\n        df = pd.read_csv(file_path, nrows=100)\n\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n\n            # Determine lookup logic\n            if col == \"id\" and \"main\" in table_name:\n                is_lookup = \"PRIMARY KEY\"\n            elif col == \"parent_id\":\n                is_lookup = f\"FOREIGN KEY → {dataset_id}_main.id\"\n            else:\n                is_lookup = \"\"\n\n            schema_rows.append({\n                \"table_name\": table_name,\n                \"column_name\": col,\n                \"data_type\": dtype,\n                \"is_lookup\": is_lookup\n            })\n\n# Save schema summary to CSV\nschema_df = pd.DataFrame(schema_rows)\nschema_df.to_csv(f\"{dataset_folder}_schema_summary.csv\", index=False)\n\nprint(f\"Saved schema summary to {dataset_folder}_schema_summary.csv\")\n\nSaved schema summary to flattened_dataset01_schema_summary.csv"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Appendix\nLinks to Sources\n\n\n\n\nData Schema\nView the primary fields and table structure.\n\n\nIPEDS Reference Files\nView the IPEDS source files and methodology.\n\n\nMSI Reference Files\nView the MSI source files and methodology.\n\n\nReplication Code\nAccess the replication code here.\n\n\nREADME File\nAccess the README file associated with replication code here.",
    "crumbs": [
      "Appendices"
    ]
  },
  {
    "objectID": "conclusions.html#expanding-dataset-usage",
    "href": "conclusions.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Expanding Dataset Usage",
    "text": "Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database."
  },
  {
    "objectID": "conclusions.html#next-steps",
    "href": "conclusions.html#next-steps",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Next Steps",
    "text": "Next Steps"
  },
  {
    "objectID": "appendices/app_msi.html",
    "href": "appendices/app_msi.html",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "",
    "text": "The Minority-Serving Institutions (MSI) Data captures institutions eligible for federal MSI programs, as determined by the U.S. Department of Education and other sources. In this project, MSI data is used to analyze institutional characteristics in relation to research publications identified in Scopus, OpenAlex, and Dimensions.\nThis dataset provides information on institutions eligible or potentially eligible for at least one MSI designation and includes data from 2017 to 2023. The MSI dataset integrates information from multiple sources and is merged with IPEDS data to track research output across different institution types.",
    "crumbs": [
      "Appendices",
      "Cleaning IPEDS and MSI Data",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#summary-of-the-msi-data",
    "href": "appendices/app_msi.html#summary-of-the-msi-data",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Summary of the MSI data",
    "text": "Summary of the MSI data\n\nTotal Institutions by MSI StatusPrivate Institutions by MSI StatusPrivate For-Profit Institutions by MSI StatusPrivate Not-for-Profit Institutions by MSI StatusPublic Institutions by MSI StatusPublic 2-year Institutions by MSI StatusPublic 4-year Institutions by MSI StatusPrivate 2-year Institutions by MSI StatusPrivate 4-year Institutions by MSI Status\n\n\n\n\n\nYear\nTotal\nMSI %\nNot_MSI %\n\n\n\n\n2017\n5242\n8.13\n91.87\n\n\n2018\n5242\n7.69\n92.31\n\n\n2019\n5242\n7.84\n92.16\n\n\n2020\n5242\n7.80\n92.20\n\n\n2021\n5242\n8.81\n91.19\n\n\n2022\n5242\n16.73\n83.27\n\n\n2023\n5242\n16.58\n83.42\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1837\n5.66\n94.34\n\n\n2018\n1837\n5.72\n94.28\n\n\n2019\n1837\n5.72\n94.28\n\n\n2020\n1837\n5.77\n94.23\n\n\n2021\n1837\n5.88\n94.12\n\n\n2022\n1837\n14.81\n85.19\n\n\n2023\n1837\n14.81\n85.19\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1409\n0.00\n100.00\n\n\n2018\n1409\n0.00\n100.00\n\n\n2019\n1409\n0.00\n100.00\n\n\n2020\n1409\n0.00\n100.00\n\n\n2021\n1409\n0.00\n100.00\n\n\n2022\n1409\n0.14\n99.86\n\n\n2023\n1409\n0.21\n99.79\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n26\n0.00\n100.00\n\n\n2018\n26\n0.00\n100.00\n\n\n2019\n26\n0.00\n100.00\n\n\n2020\n26\n0.00\n100.00\n\n\n2021\n26\n0.00\n100.00\n\n\n2022\n26\n3.85\n96.15\n\n\n2023\n26\n3.85\n96.15\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1678\n19.18\n80.81\n\n\n2018\n1678\n17.76\n82.24\n\n\n2019\n1678\n18.24\n81.76\n\n\n2020\n1678\n18.06\n81.94\n\n\n2021\n1678\n21.10\n78.90\n\n\n2022\n1678\n33.86\n66.15\n\n\n2023\n1678\n35.34\n64.66\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n933\n19.72\n80.28\n\n\n2018\n933\n16.93\n83.07\n\n\n2019\n933\n17.04\n82.96\n\n\n2020\n933\n17.15\n82.85\n\n\n2021\n933\n20.15\n79.85\n\n\n2022\n933\n36.66\n63.34\n\n\n2023\n933\n35.05\n64.95\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n807\n17.10\n82.90\n\n\n2018\n807\n17.35\n82.65\n\n\n2019\n807\n18.22\n81.78\n\n\n2020\n807\n17.82\n82.18\n\n\n2021\n807\n20.57\n79.43\n\n\n2022\n807\n32.09\n67.91\n\n\n2023\n807\n32.96\n67.04\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n139\n2.88\n97.12\n\n\n2018\n139\n2.88\n97.12\n\n\n2019\n139\n2.16\n97.84\n\n\n2020\n139\n2.16\n97.84\n\n\n2021\n139\n2.88\n97.12\n\n\n2022\n139\n12.23\n87.77\n\n\n2023\n139\n12.23\n87.77\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1714\n5.83\n94.17\n\n\n2018\n1714\n5.84\n94.16\n\n\n2019\n1714\n5.95\n94.05\n\n\n2020\n1714\n6.01\n93.99\n\n\n2021\n1714\n6.07\n93.93\n\n\n2022\n1714\n14.88\n85.12\n\n\n2023\n1714\n14.88\n85.12",
    "crumbs": [
      "Appendices",
      "Cleaning IPEDS and MSI Data",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#data-processing-and-standardization",
    "href": "appendices/app_msi.html#data-processing-and-standardization",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Data Processing and Standardization",
    "text": "Data Processing and Standardization\n\nVariable Name Changes and Formatting:\n\nyear was added to track MSI status over time.\nUNITID is the primary key to merge MSI and IPEDS datasets.\n\nHandling Missing Data and Filters:\n\nInstitutions without UNITID were excluded.\nNon-relevant columns were removed.\n\nMerging Strategy:\n\nThe 2017-2021 MSI data was directly compatible with IPEDS.\nThe 2022-2023 MSI data required additional formatting before merging.",
    "crumbs": [
      "Appendices",
      "Cleaning IPEDS and MSI Data",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#how-to-merge-with-ipeds-data",
    "href": "appendices/app_msi.html#how-to-merge-with-ipeds-data",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "How to Merge with IPEDS Data",
    "text": "How to Merge with IPEDS Data\nThe MSI dataset can be linked with IPEDS data using the UNITID and year variables. This allows for:\n\nTracking institutional MSI status over time.\nComparing MSI and non-MSI institutions within Scopus, OpenAlex, and Dimensions.\nAssessing dataset usage patterns across different institution types.",
    "crumbs": [
      "Appendices",
      "Cleaning IPEDS and MSI Data",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#footnotes",
    "href": "appendices/app_msi.html#footnotes",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 2017-2021 MSI data was sourced from the Minority Serving Institutions (MSI) Data Project by Nguyen et al. (2023), which merges the U.S. Department of Education’s MSI eligibility metrics (2017-2021) with IPEDS data.↩︎\nThe 2022-2023 MSI data was obtained from the Rutgers Graduate School of Education, which maintains annual MSI eligibility lists.↩︎\nThe 2022-2023 MSI data was obtained from the Rutgers Graduate School of Education, which maintains annual MSI eligibility lists.↩︎",
    "crumbs": [
      "Appendices",
      "Cleaning IPEDS and MSI Data",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  }
]